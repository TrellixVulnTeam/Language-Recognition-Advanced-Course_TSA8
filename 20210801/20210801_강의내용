오늘부터 8월이네요!

20210801 디렉토리에
가상환경 만들어주시고
http://naver.me/G14iFakB 에서 requirements.txt 다운받아서
필요한 라이브러리 설치해주세요


오전 데이터 수집
오후 자연어 처리

4주차 MongoDB : PDF에서 문서 추출

5~6주차 : Tensorflow 기초, Tensorflow로 하는 자연어 처리

6 ~ 10주차 : 오후 4시간씩 프로젝트 시간 

7 ~ 10주차 
 - 웹 프로그래밍 
 - 챗봇 만들기
 - OCR 해보기 (OpenCV)
 - REST API 서버 구현
 
 ===================================================================
 
데이터 분석 그리고 AI => 데이터가 없으면 의미가 없다.
 
장기과정에서 프로젝트를 하는 학생이나 데이터 분석 컨설팅 
계획이나 주제는 참 좋습니다! => 그 데이터가 있는지? 왜 의미가 있는지? 

1) 공개된 데이터를 사용하기 (kaggle, aihub,  등등)
2) web crawling : 특정 사이트의 웹페이지 내용을 가져오는 것
   web scraping : 특정 사이트에서 가져온 웹페이지에서 원하는 부분을
                  추려내는 것 
   => 문제점 : 비공식적 방법이다!
       1) 웹페이지 내용이 오늘과 내일이 다를 수 있습니다.
           => 어제까지 잘 추출하던 내용이 오늘 없을 수가 있다.
       2) 웹페이지의 내용은 해당 사이트에 존재 컨텐츠
       3) 해당 서버에 부담을 준다
           => ban을 당할 수가 있다.
           => 어제 예재의 경우 1초에 1페이지씩 처리가 되었다.
              사회공학적으로 보았을 때 사람이라면 1초에 1페이지씩
              이동한더는 것은 불가능하고도 가정 
           => 이런 패턴을 해당 사이트에서는 감시를 한다. 이상한
              동작이 발생하면 IP 주소를 차단
              [1] 해당 페이지 처리간에 지연을 준다 2초 정도 2654 * 2 
              [2] proxy 서버를 통해 우회해서 나를 속이는 경우
              [3] 헤더 부분을 변경
              [4] browserless 방식으로 웹 접속 
3) REST API : 공식적인 방법!
              REST API라는 endpoint라는 것을 사용합니다.
              endpoint제공하는 서버가 별도로 존재합니다.
              돈이 든다. 서비스화가 필요!
              사례) 
                - 날씨 - https://openweathermap.org/ 
                - 언어 확인 - https://languagelayer.com/
                - 공공데이터 - data.go.kr
                
   [특징]
   1. 공식적인 API (Application Programming Interface)가 존재
      -> 이것만 따라하면 이상없이 데이터를 얻을 수 있다.
      -> sample program도 제공
      -> 공식적인 문서들이 존재
   2. 사용량만큼 돈을 내야하는 서비스들이 대부분.
      -> 사용자가 누구인지 파악하는 API Key가 존재!
      -> 만약에 내가 API 서비스를 사용하는 프로그램을 github나
         누군가에게 공개할 때, API Key를 삭제하고 제공해야한다.
   3. API의 버전이 존재!
      -> 공식적인 서버에서 제공하는 데이터가 더 많아 질 수도 있다.
      -> 저 좋은 방식으로 데이터 결과를 주고 싶은 경우도 있다.
      -> 공식적인 서비스이기 때문에, 최소 1개월 전에는 사용자들에게
         알림을 전달
   4. API 호출의 결과는 대부분 JSON 형식을 사용!
         
사이트 가입 및 프로그램 다운로드 하시고, 
휴식 후 10시 10분에 시작하겠습니다.

         
1시간 20분만에 
1) REST API 사용법을 배웠고
2) JSON 처리 및 파이썬 딕셔너리 방법도 배웠고,
3) SQLite3를 활용한 데이터베이스 개념을 간단히 알아보고,
   간단한 데이터 입력만 해보고, dbsqlitebrowser를 통해서, 데이터를 확인

휴식 후, 13시55분에 시작하겠습니다.
SQL 기초 의견 대화창에 남겨주세요. 인원 수 확인하고 별도로
시간을 낼지 생각해 보겠습니다.

   
nltk, spacy, textblob, wordnet 
 
휴식 및 도전 문제 해결 후 15시00분에 시작하겠습니다.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 