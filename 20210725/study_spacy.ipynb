{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# 언어 모델 다운로드\r\n",
    "!python -m spacy download en_core_web_md"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import spacy\r\n",
    "import en_core_web_sm\r\n",
    "nlp = en_core_web_sm.load()\r\n",
    "doc = nlp(\"I went there\")\r\n",
    "\r\n",
    "for token in doc:\r\n",
    "    print(token, type(token), token.text, type(token.text))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import spacy\r\n",
    "import en_core_web_sm\r\n",
    "nlp = en_core_web_sm.load()\r\n",
    "doc = nlp(\"I own a pretty cat.\")\r\n",
    "\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['I', 'own', 'a', 'pretty', 'cat', '.'] <class 'list'>\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import spacy\r\n",
    "import en_core_web_sm\r\n",
    "nlp = en_core_web_sm.load()\r\n",
    "doc = nlp(\"It's been a crazy week!!!\")\r\n",
    "\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# custom tokenizer\r\n",
    "import spacy\r\n",
    "from spacy.symbols import ORTH # orthography를 의미 (맞춤법)\r\n",
    "\r\n",
    "import en_core_web_sm\r\n",
    "nlp = en_core_web_sm.load()\r\n",
    "doc = nlp(\"lemme that\")\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))\r\n",
    "\r\n",
    "special_case1 = [ {ORTH: \"lem\"}, {ORTH: \"me\"} ]\r\n",
    "special_case2 = [ {ORTH: \"Lem\"}, {ORTH: \"me\"} ]\r\n",
    "nlp.tokenizer.add_special_case(\"lemme\", special_case1)\r\n",
    "nlp.tokenizer.add_special_case(\"Lemme\", special_case2)\r\n",
    "doc = nlp(\"lemme that!!!\")\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))\r\n",
    "\r\n",
    "doc = nlp(\"Let's try again! Lemme that!, lemme\")\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# custom tokenizer - 문장기호도 custom tokenizer에 포함될 수 있는 경우\r\n",
    "import spacy\r\n",
    "from spacy.symbols import ORTH # orthography를 의미 (맞춤법)\r\n",
    "\r\n",
    "special_case = [ {ORTH: \"...lemme...?\"} ]\r\n",
    "nlp.tokenizer.add_special_case(\"...lemme...?\", special_case)\r\n",
    "doc = nlp(\"I have a dream. ...lemme...?\")\r\n",
    "print ([ token.text for token in doc ], type([ token.text for token in doc ]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# tokenizer의 debugging\r\n",
    "import spacy\r\n",
    "import en_core_web_sm\r\n",
    "nlp = en_core_web_sm.load()\r\n",
    "\r\n",
    "text = \"Let's go! Lemme\"\r\n",
    "doc = nlp(text)\r\n",
    "print ([ token.text for token in doc ])\r\n",
    "\r\n",
    "detail_tokens = nlp.tokenizer.explain(text) \r\n",
    "for detail_token in detail_tokens:\r\n",
    "    print(detail_token[1], \"\\t\", detail_token[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Sentence segmentation은 tokenization보다 좀더 복잡한 작업\r\n",
    "import spacy\r\n",
    "import en_core_web_sm\r\n",
    "nlp = en_core_web_sm.load()\r\n",
    "text = \"부산 해운대해수욕장에서 중학생 3명이 물놀이를 하던 중 1명이 실종되고 1명이 숨지는 사고가 발생했다. 25일 경찰과 소방당국에 따르면 이날 오전 3시 41분께 부산 해운대해수욕장에서 중학생 3명이 물놀이 하던 중 실종됐다는 신고가 접수됐다.\"\r\n",
    "doc = nlp(text)\r\n",
    "for sentence in doc.sents:\r\n",
    "    print(sentence)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# lemma : token의 기본 형태 (base form), 사전에서 token의 기본형으로 찾을 수 있다.\r\n",
    "# eating의 lemma => eat / eats의 lemma => eat / ate의 lemma => eat\r\n",
    "# lemmatization : token을 자신의 lemma로 찾아가는 과정 \r\n",
    "\r\n",
    "import spacy\r\n",
    "import en_core_web_sm\r\n",
    "nlp = en_core_web_sm.load()\r\n",
    "text = \"I went there for working and worked for 3 years.\"\r\n",
    "doc = nlp(text)\r\n",
    "for token in doc:\r\n",
    "    print(token.text, \"\\t\", token.lemma_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import spacy\r\n",
    "from spacy.symbols import ORTH, LEMMA\r\n",
    "import en_core_web_sm\r\n",
    "nlp = en_core_web_sm.load()\r\n",
    "special_case = [ {ORTH: \"Angeltown\", LEMMA: \"Los Angeles\"} ]\r\n",
    "nlp.tokenizer.add_special_case(\"Angeltown\", special_case)\r\n",
    "\r\n",
    "doc = nlp(\"I am flying to Angeltown\")\r\n",
    "for token in doc:\r\n",
    "    print(token.text, token.lemma_)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import spacy\r\n",
    "import en_core_web_sm\r\n",
    "nlp = en_core_web_sm.load()\r\n",
    "doc = nlp(\"I know that you have been to Korea.\")\r\n",
    "for token in doc:\r\n",
    "    print(token)\r\n",
    "\r\n",
    "print(doc[2:4])\r\n",
    "print(doc[4:])\r\n",
    "print(doc[3:-1])\r\n",
    "print(doc[6:])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import spacy\r\n",
    "import en_core_web_sm\r\n",
    "nlp = en_core_web_sm.load()\r\n",
    "doc = nlp(\"I know that you have been to Korea.\")\r\n",
    "span = doc[2:4]\r\n",
    "for token in span:\r\n",
    "    print(token)\r\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import spacy\r\n",
    "import en_core_web_sm\r\n",
    "nlp = en_core_web_sm.load()\r\n",
    "doc = nlp(\"Hello, hi!\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc[0].lower_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc = nlp(\"HELLO, Hello, hello, hEllo\")\r\n",
    "for token in doc:\r\n",
    "    print(token.text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(doc[0].is_upper)\r\n",
    "print(doc[0].is_lower)\r\n",
    "print(doc[1].is_upper)\r\n",
    "print(doc[1].is_lower)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc = nlp(\"Cat and Cat123\")\r\n",
    "print(doc[0].is_alpha)\r\n",
    "print(doc[1].is_alpha)\r\n",
    "print(doc[2].is_alpha) # nonalphabetic에는 숫자, 기호, 공백 문자를 포함"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc = nlp(\"English and 한글!\")\r\n",
    "print(doc[0].is_ascii)\r\n",
    "print(doc[2].is_ascii)\r\n",
    "print(doc[3].is_ascii)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc = nlp(\"Cat Cat123 123\")\r\n",
    "print(doc[0].is_digit)\r\n",
    "print(doc[1].is_digit)\r\n",
    "print(doc[2].is_digit)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc = nlp(\"Hey, You and me!\")\r\n",
    "print(doc[1].is_punct)\r\n",
    "print(doc[4].is_punct)\r\n",
    "print(doc[5].is_punct)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "doc = nlp(\"([ He said yes. ])\")\r\n",
    "print(doc[0])\r\n",
    "print(doc[0].is_left_punct)\r\n",
    "print(doc[1])\r\n",
    "print(doc[1].is_left_punct)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(\n",
      "True\n",
      "[\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "#길이 상관없이 space는 True로 인식\r\n",
    "doc = nlp(\" \")\r\n",
    "print(doc[0])\r\n",
    "print(len(doc[0]))\r\n",
    "print(doc[0].is_space)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " \n",
      "1\n",
      "True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# is_bracket : {} [] () 유무 확인\r\n",
    "doc = nlp(\"( You said [1] and {2} is not applicable.)\")\r\n",
    "print(doc[0].is_bracket, doc[-1].is_bracket)\r\n",
    "print(doc[3].is_bracket, doc[5].is_bracket)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True True\n",
      "True True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# is_quote : 인용문자 확인\r\n",
    "doc = nlp(\"( You said '1\\\"' is not applicable.)\")\r\n",
    "print(doc[3], doc[3].is_quote)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "' True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# is_currency : 화폐문자 유무 확인\r\n",
    "doc = nlp(\"I paid $12 for the t-shirt\")\r\n",
    "print(doc[2], doc[2].is_currency)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "$ True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# like_num : 문자열이 숫자를 의미하는지 확인, 숫자와 숫자를 나타내는 글자 모두 인식\r\n",
    "doc = nlp(\"I emailed you at leat thousand times\")\r\n",
    "print(doc[-2], doc[-2].like_num)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "thousand True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# like_email : 문자열이 이메일 형태인지 확인\r\n",
    "doc = nlp(\"My email is kaikim98@naver.com and you can visit me at http://www.naver.com any time you want\")\r\n",
    "print(doc[3], doc[3].like_email)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "kaikim98@naver.com True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "# like_url : url형태인지 확인\r\n",
    "doc = nlp(\"My email is kaikim98@naver.com and you can visit me at http://www.naver.com any time you want\")\r\n",
    "print(doc[-5], doc[-5].like_url)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "http://www.naver.com True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# shape_ : 단어들이 가진 형태를 찾아주는 기능 (X: 대문자, x: 소문자, d: 숫자)\r\n",
    "# token의 orthographic 특징을 나타내는 문자열을 출력 => 머신러닝 알고리즘에서 문자열의 featuer을 부여하고 할 때 사용가능\r\n",
    "doc = nlp(\"Girl called Kathy has a nickname Cat123.\")\r\n",
    "for token in doc:\r\n",
    "    print(token.text, token.lemma_, token.shape_)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Girl girl Xxxx\n",
      "called call xxxx\n",
      "Kathy Kathy Xxxxx\n",
      "has have xxx\n",
      "a a x\n",
      "nickname nickname xxxx\n",
      "Cat123 Cat123 Xxxddd\n",
      ". . .\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# is_oov : oov(Out of Vocabulary) 특정 토큰이 vocabulary 안에 있는지 확인\r\n",
    "doc = nlp(\"I visited Jenny at Korean Resort\")\r\n",
    "for token in doc:\r\n",
    "    print(token, token.is_oov)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I True\n",
      "visited True\n",
      "Jenny True\n",
      "at True\n",
      "Korean True\n",
      "Resort True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# is_stop: stop word(불용어 확인)\r\n",
    "doc = nlp(\"I just want to inform you that I was with the principle\")\r\n",
    "for token in doc:\r\n",
    "    print(token, token.is_stop)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "I True\n",
      "just True\n",
      "want False\n",
      "to True\n",
      "inform False\n",
      "you True\n",
      "that True\n",
      "I True\n",
      "was True\n",
      "with True\n",
      "the True\n",
      "principle False\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('venv_20210725': venv)"
  },
  "interpreter": {
   "hash": "471ea15e84e2dd4595f6db37eeacb62c30399656b4cb052a6d9439208c6a3241"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}